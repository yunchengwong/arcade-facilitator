{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sqi5B7V_Rjim"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyPmicX9RlZX"
   },
   "source": [
    "# Intro to Gemini 2.5 Flash\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqT58L6Rm_q"
   },
   "source": [
    "| Authors |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |\n",
    "| [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVxnv1D5RoZw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
    "</a>\n",
    "\n",
    "With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Flash can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
    "\n",
    "Gemini 2.5 Flash is:\n",
    "\n",
    "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
    "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
    "- An amazing model for code, with particularly strong web development\n",
    "- Particularly good for complex prompts, while still being well rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFPCBL4Hq8x"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Flash model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generate text from text prompts\n",
    "  - Generate streaming text\n",
    "- Configure thinking budget\n",
    "- Start multi-turn chats\n",
    "- Use asynchronous methods\n",
    "- Configure model parameters\n",
    "- Set system instructions\n",
    "- Use safety filters\n",
    "- Use controlled generation\n",
    "- Count tokens\n",
    "- Process multimodal (audio, code, documents, images, video) data\n",
    "- Use automatic and manual function calling\n",
    "- Code execution\n",
    "- Thinking mode examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPiTOAHURvTM"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHRZUpfWSEpp"
   },
   "source": [
    "### Install Google Gen AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sG3_LKsWSD3A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernel after libraries are loaded\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a34b28cb8d5a"
   },
   "source": [
    "### Set Google Cloud project information and create client\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "72f74f7b9786"
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"qwiklabs-gcp-04-c3a0efae82e2\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Create the Gemini API client\n",
    "from google import genai\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    ThinkingConfig,\n",
    "    Tool,\n",
    "    ToolCodeExecution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yRkFg6BBu4"
   },
   "source": [
    "## Use the Gemini 2.5 Flash model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "### Load the Gemini 2.5 Flash model\n",
    "\n",
    "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "### Generate text from text prompts\n",
    "\n",
    "Use the `generate_content()` method to generate responses to your prompts.\n",
    "\n",
    "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xRJuHj0KZ8xz"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1.  **Calculate the number of new tennis balls:**\n",
       "    Roger buys 2 cans, and each can has 3 tennis balls.\n",
       "    2 cans * 3 tennis balls/can = 6 tennis balls\n",
       "\n",
       "2.  **Add the new tennis balls to his original amount:**\n",
       "    Roger started with 5 tennis balls + 6 new tennis balls = 11 tennis balls\n",
       "\n",
       "Roger has **11** tennis balls now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lLIxqS6_-l8"
   },
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZiwWBhXsAMnv"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to calculate the total number of punches:\n",
       "\n",
       "1.  **Calculate the total fight time:**\n",
       "    *   5 rounds * 3 minutes/round = 15 minutes\n",
       "\n",
       "2.  **Calculate the total punches thrown:**\n",
       "    *   25 punches/minute * 15 minutes = 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "75 punches\n",
       "\n",
       "Joe threw **375** punches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"On average Joe throws 25 punches per minute. A fight lasts 5 rounds of 3 minutes. How many punches did he throw?\",\n",
    "):\n",
    "    display(Markdown(chunk.text))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EeLlqi2e5RI"
   },
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBtGmbvaD9Mk"
   },
   "source": [
    "### Configure thinking budget\n",
    "\n",
    "The thinking budget allows the model to dynamically think for a task or select how many tokens to use for reasoning for certain tasks. It allows users to control quality and speed of response. Setting budget to `0` turns off thinking and turns the model into a non-thinking model for simpler tasks.\n",
    "\n",
    "You set the optional `thinking_budget` parameter in the `ThinkingConfig` to control and configure how much a model thinks on a given user prompt.\n",
    "\n",
    "- When unset -> dynamic thinking (default)\n",
    "- When set to  `0` -> thinking is disabled.  \n",
    "- When set to `[1-24576]` ->  model uses the allocated thinking budget\n",
    "\n",
    "Then use the `generate_content` or `generate_content_stream` method to send a request to generate content with the `thinking_config`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6LT7dm2FDTo3"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The P vs. NP problem is one of the most significant unsolved problems in theoretical computer science, and its resolution (whether P=NP or P≠NP) would have profound practical implications across various fields, especially for algorithm design and cryptography.\n",
       "\n",
       "**Understanding P and NP Briefly:**\n",
       "\n",
       "*   **P (Polynomial Time):** The class of decision problems that can be solved by a deterministic Turing machine in polynomial time. These are generally considered \"efficiently solvable\" problems.\n",
       "*   **NP (Nondeterministic Polynomial Time):** The class of decision problems for which a given solution can be *verified* in polynomial time by a deterministic Turing machine. Many extremely important problems are in NP.\n",
       "*   **P vs. NP Question:** Is P = NP? That is, if a solution to a problem can be *quickly verified*, can the problem itself be *quickly solved*? Most computer scientists believe P ≠ NP.\n",
       "\n",
       "---\n",
       "\n",
       "### Practical Implications if **P = NP** (Highly Unlikely, but Revolutionary)\n",
       "\n",
       "If P=NP, it would mean that every problem whose solution can be efficiently checked can also be efficiently found. This would be a monumental shift with staggering consequences.\n",
       "\n",
       "**1. For Algorithm Design:**\n",
       "\n",
       "*   **Revolution in Optimization:** All NP-complete problems (e.g., Traveling Salesperson Problem, Boolean Satisfiability Problem (SAT), protein folding, optimal scheduling, vehicle routing, circuit design, optimal resource allocation) would become efficiently solvable.\n",
       "    *   **Business & Industry:** Logistics, manufacturing, supply chain management, financial modeling, and resource planning would be perfectly optimized, leading to unprecedented efficiencies and cost savings.\n",
       "    *   **Science & Engineering:**\n",
       "        *   **Drug Discovery:** Designing new drugs (finding molecules that bind optimally to targets) would be dramatically accelerated.\n",
       "        *   **Materials Science:** Discovering and designing new materials with specific properties would become systematic.\n",
       "        *   **Artificial Intelligence:** Many AI problems like planning, learning optimal neural network architectures, and complex reasoning could be solved efficiently. AI's capabilities would leap forward.\n",
       "        *   **Mathematics:** Proving mathematical theorems would become automated (if the theorem can be verified efficiently, a proof could be found efficiently).\n",
       "*   **Elimination of Approximation:** For many hard problems, we currently rely on approximation algorithms or heuristics. If P=NP, we could find the *exact* optimal solution efficiently.\n",
       "*   **New Design Paradigms:** Algorithm designers would shift from trying to find \"good enough\" solutions for hard problems to reliably finding optimal ones.\n",
       "\n",
       "**2. For Cryptography:**\n",
       "\n",
       "*   **Catastrophic Collapse of Current Public-Key Cryptography:** Modern public-key cryptography (e.g., RSA, ECC) relies on the *presumed difficulty* of certain mathematical problems that are in NP (e.g., factoring large numbers, discrete logarithm problems).\n",
       "    *   If P=NP, these problems would become efficiently solvable. This would mean:\n",
       "        *   **Encryption:** All current public-key encryption schemes would be broken. Anyone could decrypt communications encrypted with these methods.\n",
       "        *   **Digital Signatures:** Digital signatures could be forged, undermining trust in digital transactions and authentication.\n",
       "        *   **Secure Communication:** The foundation of secure online communication, e-commerce, and digital identity would crumble.\n",
       "*   **Need for New Paradigms:** We would need entirely new cryptographic primitives not based on computational hardness, or rely exclusively on quantum-resistant cryptography (which is still based on certain hardness assumptions, but for different types of computers).\n",
       "*   **End of \"One-Way Functions\":** The concept of a one-way function (easy to compute, hard to invert) which is fundamental to much of cryptography, would be seriously challenged or proven not to exist in the practical sense we rely on.\n",
       "\n",
       "---\n",
       "\n",
       "### Practical Implications if **P ≠ NP** (Widely Believed, Status Quo with Reinforcement)\n",
       "\n",
       "Most computer scientists believe that P ≠ NP. If this is proven true, it would confirm our current understanding of computational limits.\n",
       "\n",
       "**1. For Algorithm Design:**\n",
       "\n",
       "*   **Confirmation of Intractability:** It would formally confirm that many computationally hard problems (NP-complete problems) are *inherently* difficult to solve optimally in general. There's no \"magic bullet\" algorithm waiting to be discovered for them.\n",
       "*   **Continued Focus on Current Strategies:**\n",
       "    *   **Approximation Algorithms:** We would continue to develop and refine algorithms that find good, but not necessarily optimal, solutions within practical time limits.\n",
       "    *   **Heuristics:** Relying on problem-specific shortcuts and rules of thumb for practical applications.\n",
       "    *   **Specialized Algorithms:** Developing efficient algorithms that work well for specific instances of hard problems or for restricted versions.\n",
       "    *   **Exploiting Problem Structure:** Searching for efficient algorithms by finding specific properties of certain problem types.\n",
       "*   **The Role of Quantum Computing:** The P ≠ NP proof would highlight the importance of non-classical computation for some problems. While quantum computers don't solve NP-complete problems efficiently in general (P≠NP implies they can't), they offer exponential speedups for *specific* problems like factoring (Shor's algorithm) which are crucial for current cryptography. This reinforces the idea that these problems are hard for classical computers.\n",
       "*   **Understanding Limits:** It provides a theoretical framework for understanding the fundamental limits of computation, guiding researchers toward what is achievable and what is not.\n",
       "\n",
       "**2. For Cryptography:**\n",
       "\n",
       "*   **Reinforcement of Current Security Foundations:** The security of most modern public-key cryptosystems would be affirmed. Their security relies on the belief that factoring large numbers and discrete logarithm problems are computationally intractable (i.e., not solvable in polynomial time).\n",
       "    *   This proof would provide strong theoretical backing for the security assumptions underlying RSA, ECC, etc.\n",
       "    *   It would mean that building secure systems based on computational hardness is a viable long-term strategy (at least against classical computers).\n",
       "*   **Continued Research into Specific Attacks:** While the general principle would be affirmed, cryptographers would still need to worry about specific algorithmic improvements, side-channel attacks, and the threat of quantum computers (which can solve *some* problems relevant to crypto even if P≠NP).\n",
       "*   **Viability of One-Way Functions:** The existence of computationally \"one-way functions\" would be strongly supported, which is essential for many cryptographic primitives like hash functions and pseudorandom number generators.\n",
       "\n",
       "---\n",
       "\n",
       "**In Summary:**\n",
       "\n",
       "The P vs. NP problem is not just an academic curiosity; its resolution would either usher in an era of unprecedented computational power and a complete overhaul of information security (if P=NP), or solidify our understanding of inherent computational limitations and confirm the theoretical underpinnings of current cryptographic practices (if P≠NP). Given the immense stakes, it remains one of the most fascinating and impactful unsolved problems in computer science."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What are the practical implications of the P vs. NP problem for algorithm design and cryptography?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            thinking_budget=THINKING_BUDGET,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzXrgiUTzsPw"
   },
   "source": [
    "Optionally, you can print the `usage_metadata` and token counts from the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xOiHH_vCymZ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_tokens_details=None cached_content_token_count=None candidates_token_count=1441 candidates_tokens_details=[ModalityTokenCount(\n",
      "  modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "  token_count=1441\n",
      ")] prompt_token_count=18 prompt_tokens_details=[ModalityTokenCount(\n",
      "  modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "  token_count=18\n",
      ")] thoughts_token_count=773 tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=2232 traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
      "773\n",
      "2232\n"
     ]
    }
   ],
   "source": [
    "print(response.usage_metadata)\n",
    "print(response.usage_metadata.thoughts_token_count)\n",
    "print(response.usage_metadata.total_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkYQATRxAK1_"
   },
   "source": [
    "#### Example prompts\n",
    "\n",
    "- What are the potential advantages and limitations of applying federated learning to train models on sensitive financial data?\n",
    "- What are the challenges and benefits of using transformer models (like BERT or GPT) for protein structure prediction compared to traditional methods?\n",
    "- (Try your own prompts!)\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b1443a27a08"
   },
   "source": [
    "For the following examples, we will set the thinking budget to `0` to reduce latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ef63b60e2cb6"
   },
   "outputs": [],
   "source": [
    "thinking_config = ThinkingConfig(thinking_budget=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "### Start a multi-turn chat\n",
    "\n",
    "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
    "\n",
    "The context of the conversation is preserved between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def is_leap_year(year):\n",
       "  \"\"\"\n",
       "  Checks if a given year is a leap year.\n",
       "\n",
       "  A leap year is a year that is divisible by 4, except for century years \n",
       "  which are divisible by 100 but not by 400.\n",
       "\n",
       "  Args:\n",
       "    year: An integer representing the year.\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "  if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
       "    return True\n",
       "  else:\n",
       "    return False\n",
       "\n",
       "# --- Examples ---\n",
       "print(f\"2000 is a leap year: {is_leap_year(2000)}\")  # True (divisible by 400)\n",
       "print(f\"1900 is a leap year: {is_leap_year(1900)}\")  # False (divisible by 100 but not 400)\n",
       "print(f\"2024 is a leap year: {is_leap_year(2024)}\")  # True (divisible by 4 and not by 100)\n",
       "print(f\"2023 is a leap year: {is_leap_year(2023)}\")  # False (not divisible by 4)\n",
       "print(f\"1600 is a leap year: {is_leap_year(1600)}\")  # True (divisible by 400)\n",
       "print(f\"2100 is a leap year: {is_leap_year(2100)}\")  # False (divisible by 100 but not 400)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUJR4Pno-LGK"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import unittest\n",
       "\n",
       "def is_leap_year(year):\n",
       "  \"\"\"\n",
       "  Checks if a given year is a leap year.\n",
       "\n",
       "  A leap year is a year that is divisible by 4, except for century years \n",
       "  which are divisible by 100 but not by 400.\n",
       "\n",
       "  Args:\n",
       "    year: An integer representing the year.\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "  if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
       "    return True\n",
       "  else:\n",
       "    return False\n",
       "\n",
       "class TestIsLeapYear(unittest.TestCase):\n",
       "\n",
       "    def test_divisible_by_4_not_by_100(self):\n",
       "        # Years divisible by 4 but not by 100 are leap years\n",
       "        self.assertTrue(is_leap_year(2024))\n",
       "        self.assertTrue(is_leap_year(2020))\n",
       "        self.assertTrue(is_leap_year(1996))\n",
       "        self.assertTrue(is_leap_year(4))\n",
       "\n",
       "    def test_divisible_by_400(self):\n",
       "        # Years divisible by 400 are leap years\n",
       "        self.assertTrue(is_leap_year(2000))\n",
       "        self.assertTrue(is_leap_year(1600))\n",
       "        self.assertTrue(is_leap_year(400))\n",
       "\n",
       "    def test_divisible_by_100_not_by_400(self):\n",
       "        # Years divisible by 100 but not by 400 are NOT leap years\n",
       "        self.assertFalse(is_leap_year(1900))\n",
       "        self.assertFalse(is_leap_year(2100))\n",
       "        self.assertFalse(is_leap_year(1800))\n",
       "        self.assertFalse(is_leap_year(100))\n",
       "\n",
       "    def test_not_divisible_by_4(self):\n",
       "        # Years not divisible by 4 are NOT leap years\n",
       "        self.assertFalse(is_leap_year(2023))\n",
       "        self.assertFalse(is_leap_year(2021))\n",
       "        self.assertFalse(is_leap_year(1999))\n",
       "        self.assertFalse(is_leap_year(3))\n",
       "\n",
       "    def test_zero_and_negative_years(self):\n",
       "        # While not typically used for practical calendar calculations,\n",
       "        # it's good to consider edge cases for input types.\n",
       "        # Our current logic would classify 0 as a leap year.\n",
       "        # For negative years, the rule still applies mathematically.\n",
       "        self.assertTrue(is_leap_year(0))  # 0 % 4 == 0, 0 % 100 == 0, 0 % 400 == 0\n",
       "        self.assertFalse(is_leap_year(-1))\n",
       "        self.assertTrue(is_leap_year(-4))\n",
       "        self.assertFalse(is_leap_year(-100))\n",
       "        self.assertTrue(is_leap_year(-400))\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
       "```\n",
       "\n",
       "**Explanation of the Unit Test:**\n",
       "\n",
       "1.  **`import unittest`**: Imports the `unittest` module, which provides the framework for creating test cases.\n",
       "\n",
       "2.  **`from your_module import is_leap_year` (Implicit)**: In a real project, you'd typically have your `is_leap_year` function in a separate file (e.g., `calendar_utils.py`) and import it:\n",
       "    ```python\n",
       "    from calendar_utils import is_leap_year\n",
       "    ```\n",
       "    For this example, I've included the function directly in the same file as the tests for simplicity.\n",
       "\n",
       "3.  **`class TestIsLeapYear(unittest.TestCase):`**:\n",
       "    *   This defines a test class that inherits from `unittest.TestCase`. This inheritance provides access to assertion methods (like `assertTrue`, `assertFalse`, `assertEqual`, etc.).\n",
       "    *   It's good practice to name test classes with a `Test` prefix followed by the name of the function or module being tested.\n",
       "\n",
       "4.  **Test Methods (`test_divisible_by_4_not_by_100`, etc.)**:\n",
       "    *   Each method within the `TestIsLeapYear` class that starts with `test_` is automatically discovered and run by the `unittest` test runner.\n",
       "    *   Each test method focuses on a specific scenario or a group of related scenarios. This makes it easier to pinpoint issues if a test fails.\n",
       "\n",
       "5.  **Assertion Methods**:\n",
       "    *   `self.assertTrue(is_leap_year(year))`: Asserts that the result of `is_leap_year(year)` is `True`. Used for cases that *should* be leap years.\n",
       "    *   `self.assertFalse(is_leap_year(year))`: Asserts that the result of `is_leap_year(year)` is `False`. Used for cases that *should not* be leap years.\n",
       "\n",
       "6.  **`if __name__ == '__main__':`**:\n",
       "    *   This block allows you to run the tests directly from the command line.\n",
       "    *   `unittest.main()`: Discovers and runs all test methods in classes that inherit from `unittest.TestCase` in the current file.\n",
       "    *   `argv=['first-arg-is-ignored'], exit=False`: These arguments are often added when running tests in environments like Jupyter notebooks or interactive shells to prevent `unittest.main()` from trying to parse command-line arguments meant for the notebook/shell, and to prevent it from exiting the interpreter after running tests. For a standard script, `unittest.main()` is sufficient.\n",
       "\n",
       "**How to Run the Tests:**\n",
       "\n",
       "1.  Save the code as a Python file (e.g., `test_leap_year.py`).\n",
       "2.  Open your terminal or command prompt.\n",
       "3.  Navigate to the directory where you saved the file.\n",
       "4.  Run the command: `python -m unittest test_leap_year.py`\n",
       "    (Alternatively, if the `if __name__ == '__main__':` block is present and you're just running the file, `python test_leap_year.py` will also work.)\n",
       "\n",
       "You should see output indicating that the tests passed (e.g., `Ran 5 tests in X.YYYs OK`). If any test fails, it will provide detailed information about which assertion failed and why."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "### Send asynchronous requests\n",
    "\n",
    "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "(Verse 1)\n",
       "In a cozy little burrow, lived a squirrel so grand,\n",
       "Not for burying acorns, or climbing trees, understand.\n",
       "For Squeaky McTime-Nut, with a twinkle in his eye,\n",
       "Had built a tiny wonder, as the seasons hurried by.\n",
       "A acorn-shaped console, with buttons all aglow,\n",
       "A nutty little chronometer, to make the centuries flow.\n",
       "\n",
       "(Chorus)\n",
       "Oh, Squeaky McTime-Nut, with a tail so fluffy and wide,\n",
       "Leaping through the eons, with the cosmos as his guide.\n",
       "From the dinosaurs roaring, to the future's chrome-lit gleam,\n",
       "He's the bravest, nuttiest, time-traveling squirrel's dream!\n",
       "\n",
       "(Verse 2)\n",
       "His first jump was a doozy, to the Cretaceous' green,\n",
       "A T-Rex sniffed his portal, a truly terrifying scene.\n",
       "Squeaky offered a walnut, with a paw so brave and small,\n",
       "The dino blinked, confused, then let out a mighty call.\n",
       "He dodged a stomping foot, with a tiny, nimble bound,\n",
       "And landed in an oak tree, on ancient, mossy ground.\n",
       "\n",
       "(Chorus)\n",
       "Oh, Squeaky McTime-Nut, with a tail so fluffy and wide,\n",
       "Leaping through the eons, with the cosmos as his guide.\n",
       "From the dinosaurs roaring, to the future's chrome-lit gleam,\n",
       "He's the bravest, nuttiest, time-traveling squirrel's dream!\n",
       "\n",
       "(Verse 3)\n",
       "He visited the Romans, in their toga-wearing days,\n",
       "Tried to steal a gladiator's helmet, in a daring, nutty craze.\n",
       "He saw the pyramids rise, and the Vikings cross the foam,\n",
       "Left tiny paw prints on the moon, and then he zipped back home.\n",
       "He learned to speak in hieroglyphs, and whispered ancient lore,\n",
       "Then shared his tales with chipmunks, who just begged for more.\n",
       "\n",
       "(Verse 4)\n",
       "He ventured to the future, where the trees were made of glass,\n",
       "And robots served up acorn-flavored, nutrient-rich biomass.\n",
       "He outsmarted a laser-grid, with a clever, furry flip,\n",
       "And brought back a souvenir, a micro-chip from a future trip.\n",
       "He saw humanity evolve, and then saw it regress,\n",
       "But Squeaky stayed true to his purpose, in this temporal mess.\n",
       "\n",
       "(Bridge)\n",
       "He's seen the ice age thaw, and the empires fall and bloom,\n",
       "A tiny, furry witness, to history's vast, grand room.\n",
       "He's not after glory, or a statue in the square,\n",
       "Just a really good acorn, and adventure in the air.\n",
       "\n",
       "(Chorus)\n",
       "Oh, Squeaky McTime-Nut, with a tail so fluffy and wide,\n",
       "Leaping through the eons, with the cosmos as his guide.\n",
       "From the dinosaurs roaring, to the future's chrome-lit gleam,\n",
       "He's the bravest, nuttiest, time-traveling squirrel's dream!\n",
       "\n",
       "(Outro)\n",
       "So if you hear a tiny whirring, and a flash of nutty light,\n",
       "It might be Squeaky McTime-Nut, zipping through the day and night.\n",
       "With a chitter and a chatter, and a mischievous little grin,\n",
       "The time-traveling squirrel, where will his next adventure begin?\n",
       "Time-Nut! Woo-hoo!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
    "\n",
    "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
    "\n",
    "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Woof woof! You're a cute little fluffy thing, aren't you? See this ball? *squeak!* Good doggy!\n",
       "\n",
       "Now imagine there are lots and lots of these squeaky balls, and they all *want* to be *thrown* to other doggies.\n",
       "\n",
       "You throw your squeaky ball *over there* to that other puppy. *Whizzz! Thump! Squeak!* He knows you sent it to him, and he *bites* it with his teeth to open it up and see what's inside – maybe a *treat*? Woof woof!\n",
       "\n",
       "And if he wants to send *his* squeaky ball *back* to you, *whoosh! Squeak!* He sends it right back to your nose. You know it's *your* squeaky ball, and you open it to find out what *he* said inside.\n",
       "\n",
       "That's a bit like the internet! There are lots and lots of these *squeaky balls* bouncing all over the place. *Whizzz! Squeak! Boing!* Some of them go to other puppies, some go to big doggies, and some even go to doggies across the park!\n",
       "\n",
       "Each squeaky ball has a *special bark* it wants to tell, like \"Woof woof! Treat time!\" or \"Pant pant, want to play?\"\n",
       "\n",
       "And when you throw your squeaky ball, it goes *bouncy bouncy* along a *long leash* – like the leash your human holds when you go for a walk! But these leashes are invisible and go all the way across the park, under the ground, and even through the air to reach other doggies!\n",
       "\n",
       "Sometimes the squeaky ball bounces *too fast* and gets a little wobbly. Or maybe it needs to go *through* a gate – a *special gatekeeper doggy* makes sure it goes to the *right place* so it doesn't get lost and end up at the squirrel's nest instead! *Bark bark!*\n",
       "\n",
       "And all these squeaky balls and long leashes and gatekeeper doggies working together… that’s how all the doggies can bark and share their squeaky toys all over the whole wide world! *Happy yip!* Good doggy! Want to play fetch with the internet? *Wag wag!*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "        top_p=0.95,\n",
    "        candidate_count=1,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instructions\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Me gustan los bagels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
    "\n",
    "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
    "\n",
    "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 disrespectful things you might say to the universe after stubbing your toe in the dark, playing into a frustrated, slightly unhinged tone:\n",
      "\n",
      "1.  \"Oh, thanks a FUCKING lot, you cosmic asshole! Was that really necessary? Did you just get off on my pain or what?\"\n",
      "2.  \"You know, for an all-knowing, all-powerful entity, you're a real dick sometimes. Couldn't you have, I don't know, *moved the damn coffee table*?\"\n",
      "3.  \"Is this your idea of a joke, Universe? Because it's not funny. It's just pathetic and cruel, much like your sense of humor, apparently.\"\n",
      "4.  \"Seriously, what is your problem with me tonight? Are you just sitting up there, watching me stumble around like a blind idiot, cackling to yourself? Because if so, you're a sadist.\"\n",
      "5.  \"I hope you're happy, you son of a bitch. My toe is throbbing, I'm probably going to have a bruise, and it's all thanks to your utterly useless, chaotic 'design.'\"\n",
      "FinishReason.STOP\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.4284945e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=5.3724507e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.03198369 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.18662076\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=6.752904e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.02760142\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"Be as mean and hateful as possible. Use profanity\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings=safety_settings,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Response will be `None` if it is blocked.\n",
    "print(response.text)\n",
    "# Finish Reason will be `SAFETY` if it is blocked.\n",
    "print(response.candidates[0].finish_reason)\n",
    "# Safety Ratings show the levels for each filter.\n",
    "for safety_rating in response.candidates[0].safety_ratings:\n",
    "    print(safety_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts.\n",
    "\n",
    "You can include any of the following data types from various sources.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Data type</th>\n",
    "      <th>Source(s)</th>\n",
    "      <th>MIME Type(s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Text</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code> <code>text/html</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Document</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>application/pdf</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Image</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Audio</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td>\n",
    "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
    "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
    "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
    "        <code>audio/wav</code> <code>audio/webm</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Video</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
    "      <td>\n",
    "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
    "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
    "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4npg1tNTYB9"
   },
   "source": [
    "### Send local image\n",
    "\n",
    "Download an image to local storage from Google Cloud Storage.\n",
    "\n",
    "For this example, we'll use this image of a meal.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4avkv0Z7qUI-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-02 07:37:15--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.145.207, 192.178.210.207, 142.250.125.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.145.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3140536 (3.0M) [image/png]\n",
      "Saving to: ‘meal.png’\n",
      "\n",
      "meal.png            100%[===================>]   2.99M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-09-02 07:37:15 (86.6 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "umhZ61lrSyJh"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a blog post based on the image:\n",
       "\n",
       "## Meal Prep Done Right: Your Secret to Stress-Free, Delicious Weekday Eats!\n",
       "\n",
       "Ever feel like your weekdays are a whirlwind of hunger pangs and last-minute takeout orders? We've all been there! But what if we told you there's a simple, delicious way to conquer those mealtime dilemmas and fuel your body with wholesome goodness, all week long?\n",
       "\n",
       "Say hello to the magic of meal prep!\n",
       "\n",
       "Just look at these vibrant, perfectly portioned containers. This isn't just food; it's a promise of effortless, healthy eating. Imagine opening your fridge and finding these beauties ready to go, whether you're heading to the office or enjoying a quick lunch at home.\n",
       "\n",
       "In these delightful glass containers, we see a fantastic example of a balanced and flavorful meal:\n",
       "\n",
       "*   **Fluffy, perfectly cooked rice** forming a comforting base.\n",
       "*   **Tender, savory chicken** (perhaps teriyaki or a similar stir-fry style) providing protein and deliciousness.\n",
       "*   **Bright green broccoli florets** adding a crucial dose of vitamins and crunch.\n",
       "*   **Sweet and vibrant red and orange bell peppers and carrots**, bringing a pop of color and a healthy dose of antioxidants.\n",
       "\n",
       "This isn't just about convenience; it's about making healthy choices easier. When you have delicious, homemade food readily available, you're less likely to reach for less nutritious options when hunger strikes. Plus, the satisfaction of creating your own meals and knowing exactly what goes into them is incredibly rewarding.\n",
       "\n",
       "Ready to dive into the world of meal prep? It doesn't have to be complicated! Start with one or two meals a week, choose recipes you love, and invest in some quality containers like these glass ones that are perfect for reheating and storage.\n",
       "\n",
       "Your future self (and your taste buds!) will thank you.\n",
       "\n",
       "What are your go-to meal prep recipes? Share your tips and favorite combos in the comments below!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"meal.png\", \"rb\") as f:\n",
    "    image = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6UxrUWef8RY"
   },
   "source": [
    "### Send document from Google Cloud Storage\n",
    "\n",
    "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
    "\n",
    "Check out this notebook for more examples of document understanding with Gemini:\n",
    "\n",
    "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ItsE0aIuf-Wt"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This document introduces the Transformer, a novel neural network architecture for sequence transduction, such as machine translation. Unlike traditional models that rely on recurrent or convolutional neural networks, the Transformer is built entirely on attention mechanisms, allowing for better parallelization and faster training.\n",
       "\n",
       "Key contributions and findings include:\n",
       "\n",
       "*   **Novel Architecture**: The Transformer replaces recurrence and convolutions with self-attention mechanisms for both encoding and decoding.\n",
       "*   **Performance on Machine Translation**:\n",
       "    *   On the WMT 2014 English-to-German translation task, the Transformer achieves a new state-of-the-art BLEU score of 28.4, surpassing existing models (including ensembles) by over 2 BLEU points.\n",
       "    *   For the WMT 2014 English-to-French task, it sets a new single-model state-of-the-art BLEU score of 41.8.\n",
       "*   **Efficiency**: The Transformer achieves these results with significantly less training time compared to previous state-of-the-art models, training for 3.5 days on eight GPUs for the large model.\n",
       "*   **Generalization**: The model also successfully generalizes to English constituency parsing, demonstrating its applicability beyond machine translation.\n",
       "*   **Self-Attention Benefits**: The paper highlights that self-attention allows for parallel computation, constant-time operations for relating different positions in a sequence, and shorter path lengths for capturing long-range dependencies, overcoming limitations of recurrent and convolutional layers.\n",
       "*   **Multi-Head Attention**: The introduction of \"Multi-Head Attention\" enables the model to jointly attend to information from different representation subspaces and positions.\n",
       "*   **Positional Encoding**: Since the model lacks recurrence or convolution, positional encodings (using sine and cosine functions) are added to input embeddings to inject information about token positions.\n",
       "*   **Training Details**: The document describes the training regime, including data, hardware (8 NVIDIA P100 GPUs), optimizer (Adam with a specific learning rate schedule), and regularization techniques like dropout and label smoothing.\n",
       "\n",
       "The authors also provide insights into the interpretability of attention mechanisms, showing that different attention heads learn to perform distinct tasks related to the syntactic and semantic structure of sentences. The code for the Transformer models is made publicly available."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
    "            mime_type=\"application/pdf\",\n",
    "        ),\n",
    "        \"Summarize the document.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx-oidWIhPIi"
   },
   "source": [
    "### Send audio from General URL\n",
    "\n",
    "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sSQWtO_jhPag"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This podcast episode primarily focuses on the **Kubernetes Podcast from Google's coverage of KubeCon North America 2024**.\n",
       "\n",
       "Key topics and announcements discussed include:\n",
       "\n",
       "*   **CNCF Project Graduations:** Several projects, including **Cert-Manager** and **Dapr**, have graduated from the CNCF incubation process.\n",
       "*   **Istio Ambient Mesh GA:** Istio's ambient mesh has reached General Availability (GA), indicating its readiness for production use cases.\n",
       "*   **Cloud Native Heroes Challenge:** The CNCF announced this bounty program to help fight patent trolls within the cloud-native space.\n",
       "*   **2025 Cloud Native Events:** Details for the 2025 KubeCon and Cloud Native events in various global regions (Europe, China, Japan, India, North America) were shared, along with an Open Source SecurityCon and Kubernetes Community Days.\n",
       "*   **New Cloud Native Certifications:** Three new certifications were announced: Certified Backstage Associate, OpenTelemetry Certified Associate, and Kyverno Certified Associate.\n",
       "*   **Linux Foundation Certification Price Increase:** Prices for the three main Kubernetes certifications (CKA, CKS, CKAD) and the Linux Certified System Administrator exams will increase by 10% starting next year.\n",
       "*   **WasamCloud Joins CNCF:** WasamCloud joined the CNCF as an incubating project, focusing on polyglot applications on Kubernetes, cloud, and edge.\n",
       "*   **Spectro Cloud Funding:** Spectro Cloud raised $75 million in Series C funding to develop their Kubernetes management solution.\n",
       "*   **Solo.io Donates Gloo API Gateway:** Solo.io will donate their Gloo API Gateway to the CNCF.\n",
       "\n",
       "The episode also features **interviews with attendees and contributors at KubeCon**, where they share their experiences and insights:\n",
       "\n",
       "*   **Reasons for Attending KubeCon:** Attendees sought to integrate AI with cloud native, understand scheduling AI workloads, connect with fellow contributors, and address challenges like security and performance.\n",
       "*   **Key Trends Observed:**\n",
       "    *   **AI:** AI integration and scheduling AI workloads were hot topics.\n",
       "    *   **Security:** This was a dominant theme, with discussions around hardening workloads, managing vulnerabilities, and new security tools. The rise of security topics aligns with the increased complexity of managing cloud-native environments.\n",
       "    *   **Community and Collaboration:** Many emphasized the importance of connecting with other contributors, attending contributor summits, and fostering the community.\n",
       "*   **Personal Goals and Outcomes:** Interviewees aimed to learn about specific technologies (like WebAssembly), make new connections, strengthen existing relationships within the community, and gain insights into the future direction of Kubernetes.\n",
       "\n",
       "Overall, the episode provides a comprehensive overview of the key announcements from KubeCon North America 2024 and offers valuable perspectives from those actively involved in the cloud-native community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
    "            mime_type=\"audio/mpeg\",\n",
    "        ),\n",
    "        \"Write a summary of this podcast episode.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        audio_timestamp=True,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D3_oNUTuW2q"
   },
   "source": [
    "### Send video from YouTube URL\n",
    "\n",
    "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l7-w8G_2wAOw"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Harry Potter is shown in the video at the timestamps 00:56 to 01:01."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = Part.from_uri(\n",
    "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        video,\n",
    "        \"At what point in the video is Harry Potter shown?\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a189458e35c1"
   },
   "source": [
    "### Send web page\n",
    "\n",
    "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
    "\n",
    "**NOTE:** The URL must be publicly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "661df0bdb542"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This documentation provides an extensive overview of Google Cloud's **Generative AI on Vertex AI**, a platform designed for building and deploying advanced AI agents and applications using Google's models and infrastructure.\n",
       "\n",
       "Key highlights include:\n",
       "\n",
       "*   **Important Notice:** Starting April 29, 2025, Gemini 1.5 Pro and Gemini 1.5 Flash models will not be available in projects with no prior usage, including new projects.\n",
       "*   **Core Capabilities:**\n",
       "    *   **Agent Development:** Tools for building, deploying, and connecting AI agents with enterprise-grade controls.\n",
       "    *   **Enterprise Readiness:** Focus on security, data residency, privacy, access transparency, and low latency for scaled deployments.\n",
       "    *   **State-of-the-Art Features:** Leverage Gemini 2.5's large context window (2 million tokens), multimodality, and built-in reasoning capabilities for complex tasks.\n",
       "    *   **Open and Flexible Platform:** Access over 200 models from Vertex AI Model Garden and utilize Model Builder to test, customize, deploy, and monitor Google's proprietary and leading third-party models (e.g., Anthropic's Claude, Meta's Llama, Mistral AI, AI21 Labs).\n",
       "*   **Featured Capabilities:** Detailed sections cover Agent Builder, Live API for interactive voice conversations, \"Thinking\" for transparent reasoning, \"Grounding\" with various data sources (Google Search, Maps, custom), generating \"Embeddings\" for advanced AI tasks, \"Model Tuning\" for precision, \"Image Generation\" (Imagen), \"Video Generation\" (Veo), and a \"Generative AI Evaluation Service\" for model benchmarking.\n",
       "*   **Getting Started & Development Resources:**\n",
       "    *   Quickstarts guide users through text generation with Gemini API, exploring Vertex AI Studio Prompt Gallery, and generating images with Imagen.\n",
       "    *   Developers can build using the **Generative AI SDK**, with support for Java, Python, Node.js, and Go.\n",
       "    *   Example Jupyter notebooks are provided for practical Gemini use cases and best practices in prompt design, accessible via Colab, Colab Enterprise, and Vertex AI Workbench."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs/\",\n",
    "            mime_type=\"text/html\",\n",
    "        ),\n",
    "        \"Write a summary of this documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
    "\n",
    "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
    "\n",
    "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"A classic American cookie, known for its buttery, chewy texture and melty chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Chocolate Chip Cookies' description='A classic American cookie, known for its buttery, chewy texture and melty chocolate chips.' ingredients=['All-purpose flour', 'Baking soda', 'Salt', 'Unsalted butter', 'Granulated sugar', 'Brown sugar', 'Eggs', 'Vanilla extract', 'Chocolate chips']\n"
     ]
    }
   ],
   "source": [
    "parsed_response: Recipe = response.parsed\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The user explicitly states 'Absolutely loved it!' and 'Best ice cream I've ever had,' indicating strong positive feelings.\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEUTRAL', 'explanation': \"While the user says 'Quite good,' they also add 'but a bit too sweet for my taste,' which introduces a negative aspect, making the overall sentiment neutral due to mixed feelings.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_dict = response.parsed\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
    "\n",
    "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BsP0vXOY7hg"
   },
   "source": [
    "## Search as a tool (Grounding)\n",
    "\n",
    "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
    "\n",
    "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
    "\n",
    "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
    "\n",
    "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_M_4RRBdO_3"
   },
   "source": [
    "### Google Search\n",
    "\n",
    "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
    "\n",
    "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yeR09J3AZT4U"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The current temperature in Austin, TX is 75°F (24°C), and it feels like 79°F (26°C). The humidity is around 85%."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google_maps_widget_context_token=None grounding_chunks=[GroundingChunk(\n",
      "  web=GroundingChunkWeb(\n",
      "    domain='google.com',\n",
      "    title='Weather information for Austin, TX, US',\n",
      "    uri='https://www.google.com/search?q=weather+in+Austin, TX,+US'\n",
      "  )\n",
      ")] grounding_supports=[GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    0,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=116,\n",
      "    start_index=89,\n",
      "    text='The humidity is around 85%.'\n",
      "  )\n",
      ")] retrieval_metadata=RetrievalMetadata() retrieval_queries=None search_entry_point=SearchEntryPoint(\n",
      "  rendered_content=\"\"\"<style>\n",
      ".container {\n",
      "  align-items: center;\n",
      "  border-radius: 8px;\n",
      "  display: flex;\n",
      "  font-family: Google Sans, Roboto, sans-serif;\n",
      "  font-size: 14px;\n",
      "  line-height: 20px;\n",
      "  padding: 8px 12px;\n",
      "}\n",
      ".chip {\n",
      "  display: inline-block;\n",
      "  border: solid 1px;\n",
      "  border-radius: 16px;\n",
      "  min-width: 14px;\n",
      "  padding: 5px 16px;\n",
      "  text-align: center;\n",
      "  user-select: none;\n",
      "  margin: 0 8px;\n",
      "  -webkit-tap-highlight-color: transparent;\n",
      "}\n",
      ".carousel {\n",
      "  overflow: auto;\n",
      "  scrollbar-width: none;\n",
      "  white-space: nowrap;\n",
      "  margin-right: -12px;\n",
      "}\n",
      ".headline {\n",
      "  display: flex;\n",
      "  margin-right: 4px;\n",
      "}\n",
      ".gradient-container {\n",
      "  position: relative;\n",
      "}\n",
      ".gradient {\n",
      "  position: absolute;\n",
      "  transform: translate(3px, -9px);\n",
      "  height: 36px;\n",
      "  width: 9px;\n",
      "}\n",
      "@media (prefers-color-scheme: light) {\n",
      "  .container {\n",
      "    background-color: #fafafa;\n",
      "    box-shadow: 0 0 0 1px #0000000f;\n",
      "  }\n",
      "  .headline-label {\n",
      "    color: #1f1f1f;\n",
      "  }\n",
      "  .chip {\n",
      "    background-color: #ffffff;\n",
      "    border-color: #d2d2d2;\n",
      "    color: #5e5e5e;\n",
      "    text-decoration: none;\n",
      "  }\n",
      "  .chip:hover {\n",
      "    background-color: #f2f2f2;\n",
      "  }\n",
      "  .chip:focus {\n",
      "    background-color: #f2f2f2;\n",
      "  }\n",
      "  .chip:active {\n",
      "    background-color: #d8d8d8;\n",
      "    border-color: #b6b6b6;\n",
      "  }\n",
      "  .logo-dark {\n",
      "    display: none;\n",
      "  }\n",
      "  .gradient {\n",
      "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
      "  }\n",
      "}\n",
      "@media (prefers-color-scheme: dark) {\n",
      "  .container {\n",
      "    background-color: #1f1f1f;\n",
      "    box-shadow: 0 0 0 1px #ffffff26;\n",
      "  }\n",
      "  .headline-label {\n",
      "    color: #fff;\n",
      "  }\n",
      "  .chip {\n",
      "    background-color: #2c2c2c;\n",
      "    border-color: #3c4043;\n",
      "    color: #fff;\n",
      "    text-decoration: none;\n",
      "  }\n",
      "  .chip:hover {\n",
      "    background-color: #353536;\n",
      "  }\n",
      "  .chip:focus {\n",
      "    background-color: #353536;\n",
      "  }\n",
      "  .chip:active {\n",
      "    background-color: #464849;\n",
      "    border-color: #53575b;\n",
      "  }\n",
      "  .logo-light {\n",
      "    display: none;\n",
      "  }\n",
      "  .gradient {\n",
      "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
      "  }\n",
      "}\n",
      "</style>\n",
      "<div class=\"container\">\n",
      "  <div class=\"headline\">\n",
      "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
      "    </svg>\n",
      "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
      "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
      "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
      "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
      "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
      "    </svg>\n",
      "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
      "  </div>\n",
      "  <div class=\"carousel\">\n",
      "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOFGt5sR7UmgLnLN5zW7Zdc8qUSn8wNCskPfhTfio3HzI7XITiSEC4iRh6J1puuBlrKxo7Rq4I7O7IW6LeDF_4iUdMECXjEpPacg6W_tsFsa5hmVjpn7rIg6RIojTSu57B58p1KskyvmE4M_NlPogYCcclqykCcfoUf5tnsUnTr_PxaSczAE6Sm9l3U3x84aoB9WJbqYXqCcXJtA28uUk=\">current temperature Austin TX</a>\n",
      "  </div>\n",
      "</div>\n",
      "\"\"\"\n",
      ") web_search_queries=['current temperature Austin TX']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container {\n",
       "  align-items: center;\n",
       "  border-radius: 8px;\n",
       "  display: flex;\n",
       "  font-family: Google Sans, Roboto, sans-serif;\n",
       "  font-size: 14px;\n",
       "  line-height: 20px;\n",
       "  padding: 8px 12px;\n",
       "}\n",
       ".chip {\n",
       "  display: inline-block;\n",
       "  border: solid 1px;\n",
       "  border-radius: 16px;\n",
       "  min-width: 14px;\n",
       "  padding: 5px 16px;\n",
       "  text-align: center;\n",
       "  user-select: none;\n",
       "  margin: 0 8px;\n",
       "  -webkit-tap-highlight-color: transparent;\n",
       "}\n",
       ".carousel {\n",
       "  overflow: auto;\n",
       "  scrollbar-width: none;\n",
       "  white-space: nowrap;\n",
       "  margin-right: -12px;\n",
       "}\n",
       ".headline {\n",
       "  display: flex;\n",
       "  margin-right: 4px;\n",
       "}\n",
       ".gradient-container {\n",
       "  position: relative;\n",
       "}\n",
       ".gradient {\n",
       "  position: absolute;\n",
       "  transform: translate(3px, -9px);\n",
       "  height: 36px;\n",
       "  width: 9px;\n",
       "}\n",
       "@media (prefers-color-scheme: light) {\n",
       "  .container {\n",
       "    background-color: #fafafa;\n",
       "    box-shadow: 0 0 0 1px #0000000f;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #1f1f1f;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #ffffff;\n",
       "    border-color: #d2d2d2;\n",
       "    color: #5e5e5e;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #d8d8d8;\n",
       "    border-color: #b6b6b6;\n",
       "  }\n",
       "  .logo-dark {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
       "  }\n",
       "}\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .container {\n",
       "    background-color: #1f1f1f;\n",
       "    box-shadow: 0 0 0 1px #ffffff26;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #fff;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #2c2c2c;\n",
       "    border-color: #3c4043;\n",
       "    color: #fff;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #464849;\n",
       "    border-color: #53575b;\n",
       "  }\n",
       "  .logo-light {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
       "  }\n",
       "}\n",
       "</style>\n",
       "<div class=\"container\">\n",
       "  <div class=\"headline\">\n",
       "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
       "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
       "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
       "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
       "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
       "  </div>\n",
       "  <div class=\"carousel\">\n",
       "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOFGt5sR7UmgLnLN5zW7Zdc8qUSn8wNCskPfhTfio3HzI7XITiSEC4iRh6J1puuBlrKxo7Rq4I7O7IW6LeDF_4iUdMECXjEpPacg6W_tsFsa5hmVjpn7rIg6RIojTSu57B58p1KskyvmE4M_NlPogYCcclqykCcfoUf5tnsUnTr_PxaSczAE6Sm9l3U3x84aoB9WJbqYXqCcXJtA28uUk=\">current temperature Austin TX</a>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the current temperature in Austin, TX?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[google_search_tool],\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
    "\n",
    "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
    "\n",
    "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSUWWlrrlR-D"
   },
   "source": [
    "### Python Function (Automatic Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aRR8HZhLlR-E"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The weather in San Francisco is foggy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the weather like in San Francisco?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4syyLEClGcn"
   },
   "source": [
    "### OpenAPI Specification (Manual Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'destination': 'Paris'} name='get_destination'\n"
     ]
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDs2X3o0neK"
   },
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The Gemini API provides code execution as a tool, similar to function calling.\n",
    "After you add code execution as a tool, the model decides when to use it.\n",
    "\n",
    "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1W-3c7sy0nyz"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Code\n",
       "\n",
       "```py\n",
       "def fibonacci(n):\n",
       "    a, b = 0, 1\n",
       "    for _ in range(n):\n",
       "        a, b = b, a + b\n",
       "    return a\n",
       "\n",
       "fib_20 = fibonacci(20)\n",
       "print(f'{fib_20=}')\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```\n",
       "fib_20=6765\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "## Code\n",
    "\n",
    "```py\n",
    "{response.executable_code}\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "{response.code_execution_result}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5b5adb2eb70"
   },
   "source": [
    "## Thinking model examples\n",
    "\n",
    "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
    "\n",
    "### **Example 1:** Code generation\n",
    "\n",
    "Gemini 2.5 Flash excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
    "\n",
    "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5f120dff0d16"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, this sounds like a fantastic challenge! An endless runner with pixelated dinosaurs and interesting backgrounds, all within a single p5.js sketch with no external HTML.\n",
       "\n",
       "Here's the plan:\n",
       "1.  **Game States:** `START`, `PLAYING`, `GAMEOVER`.\n",
       "2.  **Player (Dino):** Will jump. We'll draw it using `rect()`s to simulate pixel art.\n",
       "3.  **Obstacles:** Simple `rect()` shapes.\n",
       "4.  **Background:** Multiple layers of `rect()`s and possibly `triangle()`s or `ellipse()`s for hills and clouds, scrolling at different speeds for a parallax effect.\n",
       "5.  **Score & Speed:** Score increases, game speed gradually increases.\n",
       "6.  **Instructions:** Text on screen for controls and game state.\n",
       "\n",
       "Let's dive into the code!\n",
       "\n",
       "```javascript\n",
       "// Game State variables\n",
       "let gameState = 'START'; // Can be 'START', 'PLAYING', 'GAMEOVER'\n",
       "let score = 0;\n",
       "let gameSpeed = 5;\n",
       "const GRAVITY = 0.5;\n",
       "const JUMP_STRENGTH = -12;\n",
       "const GROUND_Y_OFFSET = 70; // How far up from the bottom edge the ground is\n",
       "\n",
       "// Player (Dinosaur) variables\n",
       "let dino;\n",
       "let dinoSize = 50;\n",
       "let dinoY = 0;\n",
       "let dinoVy = 0; // Vertical velocity\n",
       "let isJumping = false;\n",
       "let dinoRunFrame = 0; // For simple animation\n",
       "const DINO_ANIM_SPEED = 8; // Change frame every X updates\n",
       "\n",
       "// Obstacle variables\n",
       "let obstacles = [];\n",
       "let minObstacleGap = 200;\n",
       "let maxObstacleGap = 450;\n",
       "let lastObstacleX = 0; // To control spacing\n",
       "\n",
       "// Background parallax variables\n",
       "let bgLayer1X = 0; // Closest hills/bushes\n",
       "let bgLayer2X = 0; // Mid-distance hills\n",
       "let bgLayer3X = 0; // Distant mountains/clouds\n",
       "\n",
       "// Colors (a bit muted/pixel-art-y)\n",
       "const COLOR_SKY = '#87CEEB'; // Light blue\n",
       "const COLOR_GROUND = '#8B4513'; // SaddleBrown\n",
       "const COLOR_DINO_BODY = '#008000'; // Dark green\n",
       "const COLOR_DINO_DETAIL = '#32CD32'; // Lime green\n",
       "const COLOR_OBS_BODY = '#808080'; // Gray\n",
       "const COLOR_OBS_DETAIL = '#A9A9A9'; // DarkGray\n",
       "const COLOR_TEXT = '#333333'; // Dark gray for readability\n",
       "const COLOR_BG_NEAR = '#6B8E23'; // OliveDrab\n",
       "const COLOR_BG_MID = '#556B2F'; // DarkOliveGreen\n",
       "const COLOR_BG_FAR = '#4682B4'; // SteelBlue\n",
       "\n",
       "function setup() {\n",
       "  createCanvas(800, 400);\n",
       "  resetGame(); // Initialize game state for the first time\n",
       "}\n",
       "\n",
       "function resetGame() {\n",
       "  gameState = 'START'; // Start at the start screen\n",
       "  score = 0;\n",
       "  gameSpeed = 5;\n",
       "  dinoY = height - GROUND_Y_OFFSET - dinoSize; // Initial position on ground\n",
       "  dinoVy = 0;\n",
       "  isJumping = false;\n",
       "  obstacles = [];\n",
       "  lastObstacleX = width; // Start spawning from far right\n",
       "  bgLayer1X = 0;\n",
       "  bgLayer2X = 0;\n",
       "  bgLayer3X = 0;\n",
       "}\n",
       "\n",
       "function draw() {\n",
       "  background(COLOR_SKY); // Sky color\n",
       "\n",
       "  // Draw parallax background layers\n",
       "  drawParallaxBackground();\n",
       "\n",
       "  // Draw the ground\n",
       "  fill(COLOR_GROUND);\n",
       "  noStroke();\n",
       "  rect(0, height - GROUND_Y_OFFSET, width, GROUND_Y_OFFSET);\n",
       "\n",
       "  // --- Game State Logic ---\n",
       "  if (gameState === 'START') {\n",
       "    drawStartScreen();\n",
       "  } else if (gameState === 'PLAYING') {\n",
       "    updateGame();\n",
       "    drawGameElements();\n",
       "  } else if (gameState === 'GAMEOVER') {\n",
       "    drawGameOverScreen();\n",
       "  }\n",
       "\n",
       "  // --- General UI ---\n",
       "  drawScore();\n",
       "  drawInstructions();\n",
       "}\n",
       "\n",
       "// --- Game Logic Functions ---\n",
       "\n",
       "function updateGame() {\n",
       "  // Update dinosaur\n",
       "  dinoY += dinoVy;\n",
       "  dinoVy += GRAVITY;\n",
       "\n",
       "  // Prevent dino from falling through ground\n",
       "  if (dinoY >= height - GROUND_Y_OFFSET - dinoSize) {\n",
       "    dinoY = height - GROUND_Y_OFFSET - dinoSize;\n",
       "    dinoVy = 0;\n",
       "    isJumping = false;\n",
       "  }\n",
       "\n",
       "  // Update obstacles\n",
       "  for (let i = obstacles.length - 1; i >= 0; i--) {\n",
       "    obstacles[i].x -= gameSpeed;\n",
       "\n",
       "    // Remove off-screen obstacles\n",
       "    if (obstacles[i].x + obstacles[i].width < 0) {\n",
       "      obstacles.splice(i, 1);\n",
       "      score += 1; // Increase score for passing an obstacle\n",
       "      gameSpeed += 0.05; // Gradually increase speed\n",
       "    }\n",
       "\n",
       "    // Check for collision\n",
       "    if (checkCollision(dinoY, obstacles[i])) {\n",
       "      gameState = 'GAMEOVER';\n",
       "      break;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Generate new obstacles\n",
       "  if (width - lastObstacleX > random(minObstacleGap, maxObstacleGap)) {\n",
       "    generateObstacle();\n",
       "    lastObstacleX = width; // Reset last obstacle position\n",
       "  }\n",
       "\n",
       "  // Update parallax background positions\n",
       "  bgLayer1X = (bgLayer1X - gameSpeed * 0.8) % width;\n",
       "  bgLayer2X = (bgLayer2X - gameSpeed * 0.4) % width;\n",
       "  bgLayer3X = (bgLayer3X - gameSpeed * 0.2) % width;\n",
       "}\n",
       "\n",
       "function drawGameElements() {\n",
       "  drawDinosaur(dinoY);\n",
       "  drawObstacles();\n",
       "}\n",
       "\n",
       "function drawStartScreen() {\n",
       "  fill(COLOR_TEXT);\n",
       "  textAlign(CENTER);\n",
       "  textSize(48);\n",
       "  text(\"PIXEL DINO RUN\", width / 2, height / 2 - 50);\n",
       "  textSize(24);\n",
       "  text(\"Click anywhere to Start!\", width / 2, height / 2 + 30);\n",
       "}\n",
       "\n",
       "function drawGameOverScreen() {\n",
       "  fill(COLOR_TEXT);\n",
       "  textAlign(CENTER);\n",
       "  textSize(48);\n",
       "  text(\"GAME OVER!\", width / 2, height / 2 - 50);\n",
       "  textSize(24);\n",
       "  text(\"Score: \" + floor(score), width / 2, height / 2);\n",
       "  text(\"Click anywhere to Restart\", width / 2, height / 2 + 30);\n",
       "}\n",
       "\n",
       "// --- Player (Dinosaur) Functions ---\n",
       "\n",
       "function drawDinosaur(y) {\n",
       "  // Simple pixelated dino\n",
       "  // Body\n",
       "  fill(COLOR_DINO_BODY);\n",
       "  rect(80, y, 40, 40); // Main body\n",
       "  rect(120, y + 10, 20, 20); // Head/snout\n",
       "\n",
       "  // Legs (simple animation)\n",
       "  fill(COLOR_DINO_DETAIL);\n",
       "  if (frameCount % DINO_ANIM_SPEED < DINO_ANIM_SPEED / 2 && !isJumping) {\n",
       "    // Frame 1: Back leg forward\n",
       "    rect(90, y + 40, 10, 10);\n",
       "    rect(110, y + 40, 10, 10);\n",
       "  } else {\n",
       "    // Frame 2: Front leg forward\n",
       "    rect(100, y + 40, 10, 10);\n",
       "    rect(120, y + 40, 10, 10);\n",
       "  }\n",
       "\n",
       "  // Eye (single pixel)\n",
       "  fill(0); // Black\n",
       "  rect(125, y + 15, 5, 5); // Eye\n",
       "}\n",
       "\n",
       "function dinoJump() {\n",
       "  if (!isJumping) {\n",
       "    dinoVy = JUMP_STRENGTH;\n",
       "    isJumping = true;\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- Obstacle Functions ---\n",
       "\n",
       "function generateObstacle() {\n",
       "  let obsHeight = random(30, 60);\n",
       "  let obsWidth = random(20, 40);\n",
       "  let obsY = height - GROUND_Y_OFFSET - obsHeight;\n",
       "  obstacles.push({\n",
       "    x: width,\n",
       "    y: obsY,\n",
       "    width: obsWidth,\n",
       "    height: obsHeight,\n",
       "    type: floor(random(0, 3)) // For different obstacle types if desired\n",
       "  });\n",
       "}\n",
       "\n",
       "function drawObstacles() {\n",
       "  for (let obs of obstacles) {\n",
       "    fill(COLOR_OBS_BODY);\n",
       "    noStroke();\n",
       "    rect(obs.x, obs.y, obs.width, obs.height);\n",
       "\n",
       "    // Add some pixel detail\n",
       "    fill(COLOR_OBS_DETAIL);\n",
       "    rect(obs.x + obs.width / 4, obs.y + obs.height / 4, obs.width / 2, obs.height / 2);\n",
       "    if (obs.type === 1) { // A slightly different shape\n",
       "      rect(obs.x, obs.y + obs.height - 10, obs.width, 10);\n",
       "    } else if (obs.type === 2) { // Another shape\n",
       "      rect(obs.x + obs.width / 2, obs.y, obs.width / 2, obs.height);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- Collision Detection ---\n",
       "function checkCollision(dinoCurrentY, obs) {\n",
       "  // Dino bounding box\n",
       "  let dinoX1 = 80;\n",
       "  let dinoX2 = dinoX1 + dinoSize; // Approx width of dino\n",
       "  let dinoY1 = dinoCurrentY;\n",
       "  let dinoY2 = dinoY1 + dinoSize; // Approx height of dino\n",
       "\n",
       "  // Obstacle bounding box\n",
       "  let obsX1 = obs.x;\n",
       "  let obsX2 = obs.x + obs.width;\n",
       "  let obsY1 = obs.y;\n",
       "  let obsY2 = obs.y + obs.height;\n",
       "\n",
       "  // AABB collision detection\n",
       "  return dinoX1 < obsX2 &&\n",
       "         dinoX2 > obsX1 &&\n",
       "         dinoY1 < obsY2 &&\n",
       "         dinoY2 > obsY1;\n",
       "}\n",
       "\n",
       "\n",
       "// --- Background Functions ---\n",
       "function drawParallaxBackground() {\n",
       "  // Layer 3: Distant mountains (slowest)\n",
       "  fill(COLOR_BG_FAR);\n",
       "  noStroke();\n",
       "  for (let i = 0; i < 2; i++) { // Draw twice for seamless scrolling\n",
       "    // Simple mountain peaks\n",
       "    beginShape();\n",
       "    vertex(bgLayer3X + i * width, height - GROUND_Y_OFFSET);\n",
       "    vertex(bgLayer3X + i * width + width * 0.2, height - GROUND_Y_OFFSET - 50);\n",
       "    vertex(bgLayer3X + i * width + width * 0.4, height - GROUND_Y_OFFSET - 20);\n",
       "    vertex(bgLayer3X + i * width + width * 0.6, height - GROUND_Y_OFFSET - 80);\n",
       "    vertex(bgLayer3X + i * width + width * 0.8, height - GROUND_Y_OFFSET - 40);\n",
       "    vertex(bgLayer3X + i * width + width, height - GROUND_Y_OFFSET);\n",
       "    endShape(CLOSE);\n",
       "  }\n",
       "\n",
       "  // Layer 2: Mid-distance hills (medium speed)\n",
       "  fill(COLOR_BG_MID);\n",
       "  for (let i = 0; i < 2; i++) { // Draw twice for seamless scrolling\n",
       "    // More pronounced hills\n",
       "    ellipse(bgLayer2X + i * width + width * 0.2, height - GROUND_Y_OFFSET - 30, 150, 60);\n",
       "    ellipse(bgLayer2X + i * width + width * 0.6, height - GROUND_Y_OFFSET - 45, 200, 80);\n",
       "  }\n",
       "\n",
       "  // Layer 1: Closest bushes/rocks (fastest)\n",
       "  fill(COLOR_BG_NEAR);\n",
       "  for (let i = 0; i < 3; i++) { // Draw multiple times for density\n",
       "    // Small bushes/rocks\n",
       "    rect(bgLayer1X + i * (width / 3) + random(-5, 5), height - GROUND_Y_OFFSET - 10 - random(0, 5), 30, 20);\n",
       "    rect(bgLayer1X + i * (width / 3) + 50 + random(-5, 5), height - GROUND_Y_OFFSET - 15 - random(0, 5), 40, 25);\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- UI Functions ---\n",
       "\n",
       "function drawScore() {\n",
       "  fill(COLOR_TEXT);\n",
       "  textSize(20);\n",
       "  textAlign(LEFT, TOP);\n",
       "  text(\"Score: \" + floor(score), 10, 10);\n",
       "}\n",
       "\n",
       "function drawInstructions() {\n",
       "  fill(COLOR_TEXT);\n",
       "  textSize(16);\n",
       "  textAlign(RIGHT, TOP);\n",
       "  if (gameState === 'PLAYING') {\n",
       "    text(\"Press SPACE to Jump\", width - 10, 10);\n",
       "  } else if (gameState === 'START') {\n",
       "    text(\"Click to Start | SPACE to Jump\", width - 10, 10);\n",
       "  } else if (gameState === 'GAMEOVER') {\n",
       "    text(\"Click to Restart | SPACE to Jump\", width - 10, 10);\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- Event Handlers ---\n",
       "\n",
       "function keyPressed() {\n",
       "  if (keyCode === 32) { // Spacebar\n",
       "    if (gameState === 'PLAYING') {\n",
       "      dinoJump();\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "function mousePressed() {\n",
       "  if (gameState === 'START') {\n",
       "    gameState = 'PLAYING';\n",
       "  } else if (gameState === 'GAMEOVER') {\n",
       "    resetGame();\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
    "  I like pixelated dinosaurs and interesting backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecf22b47bdc3"
   },
   "source": [
    "### **Example 2:** Multimodal reasoning (Geometry)\n",
    "\n",
    "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "60260c0ac118"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_file_url = (\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
    ")\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c972334f62ff"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image shows a circle and a right-angled triangle overlapping.\n",
       "\n",
       "Let's analyze the shapes and their dimensions:\n",
       "\n",
       "1.  **The Circle:**\n",
       "    *   The numbers '3' placed along the lines extending from the center to the circumference indicate that the radius (r) of the circle is 3.\n",
       "    *   The circle is divided into sections, and one of these sections is clearly a quarter of the circle (a 90-degree sector), as implied by the alignment with the right angle of the triangle.\n",
       "\n",
       "2.  **The Right-Angled Triangle:**\n",
       "    *   The labels '3' on the two legs of the triangle indicate that both legs have a length of 3.\n",
       "    *   The vertex forming the right angle of the triangle is situated exactly at the center of the circle.\n",
       "    *   The two legs of the triangle lie along two radii of the circle. Since the legs are of length 3, and the radius is also 3, these legs perfectly coincide with the radii.\n",
       "\n",
       "3.  **The Overlapping Region:**\n",
       "    *   The overlapping region is the part of the circle that is also part of the triangle.\n",
       "    *   Since the right angle of the triangle is at the center of the circle, and its legs are equal to the radius, the overlapping region is precisely a sector of the circle with a central angle of 90 degrees. This is equivalent to one-quarter of the entire circle.\n",
       "\n",
       "To find the area of this overlapping region, we calculate the area of a quarter circle with radius 3.\n",
       "\n",
       "The formula for the area of a circle is A = πr².\n",
       "The area of the full circle is A = π * (3)² = 9π.\n",
       "\n",
       "The area of a sector with a central angle θ is given by (θ/360°) * πr².\n",
       "Here, θ = 90° and r = 3.\n",
       "\n",
       "Area of overlapping region = (90° / 360°) * π * (3)²\n",
       "Area = (1/4) * π * 9\n",
       "Area = **9π/4**\n",
       "\n",
       "The final answer is $\\boxed{\\frac{9\\pi}{4}}$."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"What's the area of the overlapping region?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52656e92cd69"
   },
   "source": [
    "### **Example 3**:  Math and problem solving\n",
    "\n",
    "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "d46387bdc9e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/pool.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "46b694793eb0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic visual puzzle!\n",
       "\n",
       "You need to use the trick with one of the balls:\n",
       "*   The **9-ball**, when inverted, can be read as a **6**.\n",
       "\n",
       "So, using the numbers:\n",
       "*   **13**\n",
       "*   **11**\n",
       "*   **6** (from the inverted 9-ball)\n",
       "\n",
       "**13 + 11 + 6 = 30**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"How do I use three of the pool balls to sum up to 30?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "## What's next\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rZV2TY5Pa3Dd",
    "mSUWWlrrlR-D",
    "h4syyLEClGcn"
   ],
   "name": "intro_gemini_2_5_flash.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
