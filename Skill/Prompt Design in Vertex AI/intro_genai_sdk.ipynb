{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-d1d9f6cd2acf\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-tiytzQE0uM"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PoF18EwhI7e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D3SI1X-JVMBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Unlock Your Weekday Wins: Delicious & Easy Meal Prep!\n",
      "\n",
      "Take a moment to soak in this image. Doesn't it just radiate efficiency and deliciousness? What you're seeing here isn't just a pretty picture of a meal; it's a snapshot of a successful, stress-free week waiting to happen, all thanks to the magic of meal prepping!\n",
      "\n",
      "We all know the weekday struggle: the morning rush, the lunch hour scramble, and the temptation of expensive, less-than-healthy takeout. But imagine this: instead of wondering what to eat, you simply reach into your fridge and pull out one of these perfectly portioned, incredibly appetizing containers. That's the power of meal prep!\n",
      "\n",
      "In these clear glass containers, we see a symphony of flavors and textures. Golden brown, savory chicken pieces, vibrant green broccoli florets, and sweet red bell peppers and carrots arranged perfectly beside a generous serving of fluffy brown rice. A sprinkle of sesame seeds and fresh green onions adds that irresistible touch, promising a meal that's both nourishing and packed with flavor, likely a delicious Asian-inspired stir-fry or teriyaki dish.\n",
      "\n",
      "**Why embrace meal prep like this?**\n",
      "\n",
      "1.  **Time Saver:** Spend a couple of hours on a Sunday, and your lunches (or dinners!) for the entire week are sorted. No more last-minute cooking or indecision.\n",
      "2.  **Budget Friendly:** Skip the pricey takeout and restaurant meals. Buying ingredients in bulk and cooking at home is significantly cheaper.\n",
      "3.  **Healthier Choices:** You're in control of your ingredients, portions, and cooking methods. Fuel your body with fresh, wholesome food tailored to your needs.\n",
      "4.  **Reduced Stress:** One less decision to make during a busy day means more mental space for what truly matters.\n",
      "\n",
      "This image is a perfect example of balanced meal prepping – protein, complex carbs, and plenty of colorful vegetables. It’s a simple, effective way to ensure you're making delicious, healthy decisions all week long.\n",
      "\n",
      "Ready to transform your weekdays? Start small, pick one meal, and get prepping. Your future self (and your wallet!) will thank you!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pG6l1Fuka6ZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Fuel Your Week: The Art of Delicious Meal Prep\n",
      "\n",
      "Ever find yourself staring into the fridge at lunchtime, wondering what to eat? Or reaching for unhealthy takeout because time just vanished? Look no further than this vibrant image, showcasing the ultimate solution to healthy, convenient eating: perfectly portioned, colorful meal prep containers.\n",
      "\n",
      "At first glance, your eyes are drawn to the beautiful organization within these clear glass containers. Fluffy, light-colored rice forms the base, a comforting foundation for what promises to be a burst of flavor. Nestled beside it, succulent pieces of what appears to be tender chicken (or perhaps a plant-based protein) are coated in a rich, savory glaze, liberally sprinkled with toasted sesame seeds and fresh green onions, hinting at a delightful Asian-inspired stir-fry or teriyaki dish.\n",
      "\n",
      "Adding a stunning pop of color and essential nutrients are the vibrant vegetables: bright green broccoli florets, perfectly steamed or lightly sautéed, stand proudly alongside strips of crisp red and orange bell peppers, and possibly carrots, adding both crunch and natural sweetness. The composition is not just appetizing; it's a testament to the \"eat the rainbow\" philosophy, ensuring a wide array of vitamins and minerals. The presence of stylish chopsticks in the foreground further emphasizes the ready-to-eat nature of these meals, while a small bowl of extra sesame seeds and a bottle of sauce in the background suggest customizability and added flavor.\n",
      "\n",
      "Beyond its tempting appearance, this image truly champions the power of meal prepping. Imagine opening your fridge mid-week to these beauties – no cooking, no decision fatigue, just a wholesome, satisfying meal ready in moments. Meal prep grants you back precious minutes, keeps you on track with your health goals, and can significantly cut down on impulse food purchases. It's about setting yourself up for success, ensuring you have nourishing options at your fingertips, even on your busiest days.\n",
      "\n",
      "So, whether you're a seasoned meal prepper or looking to dip your toes into this life-changing habit, let this picture be your inspiration. It's a vivid reminder that healthy eating doesn't have to be boring or time-consuming. It can be delicious, beautiful, and utterly convenient. Grab those containers, plan your menu, and fuel your week the smart, delicious way!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! Okay, little floof, listen up! This is all about **squeaky toys**!\n",
      "\n",
      "Imagine you have your absolute FAVORITE **squeaky toy** – that's like a picture of a squirrel, or a video of a bouncy ball, or just a little \"woof\" message you want to send.\n",
      "\n",
      "1.  **You (Your Computer/Phone):** You're a very good puppy! You have the toy. When you want to send it, you **SQUEAK IT!** (That's like typing or tapping).\n",
      "\n",
      "2.  **The Invisible Leash (Wires) or Invisible Sniffy Path (Wi-Fi):**\n",
      "    *   Sometimes, your squeaky toy goes on a long, long, *invisible leash* straight to another puppy's mouth! (That's like a wire, super fast!)\n",
      "    *   Other times, your squeaky toy just *floats through the air* on an *invisible sniffy path*! You can't see it, but your nose knows it's there! (That's Wi-Fi!)\n",
      "\n",
      "3.  **The Super Smart Squirrel (The Router):**\n",
      "    *   Now, there are SO many puppies in the world, right? How does your squeaky toy know where to go? There's a **super-duper smart squirrel** who lives in your house!\n",
      "    *   When you squeak your toy, the squirrel *sniffs it* and knows *exactly* which invisible leash or sniffy path to send it down to get to the *right* puppy. He's like the best fetch player EVER!\n",
      "\n",
      "4.  **The Giant, Super-Duper Toy Box (Servers):**\n",
      "    *   Sometimes, puppies don't just send toys to *other puppies*. Sometimes, they want to put their toys in a **GIANT, super-duper toy box** where *everyone* can come and look at them!\n",
      "    *   These giant toy boxes are in big, comfy dog beds far, far away. When you want to see *all* the toys about squirrels, you send a little \"woof\" to the giant toy box, and it sends *all* its squirrel toys back to you!\n",
      "\n",
      "5.  **The Special Sniff-Spot (Addresses/Websites):**\n",
      "    *   Every puppy has a **special sniff-spot** – like a unique smell on their collar. That's how the smart squirrel knows *exactly* which puppy to send the toy to.\n",
      "    *   And those giant toy boxes? They have special sniff-spots too! Like \"www.squirrels-are-fun.com\" – that's just a special sniff-spot for the squirrel toy box!\n",
      "\n",
      "So, in short, little floof: The internet is just a HUGE, invisible dog park where **squeaky toys** (your messages, pictures, videos) zoom around on invisible leashes and sniffy paths, guided by super smart squirrels, to get to other puppies or giant toy boxes!\n",
      "\n",
      "Now, go chase that tail! Good boy/girl!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might say:\n",
      "\n",
      "1.  \"Oh, *this* is your grand design for my evening, Universe? Real creative.\"\n",
      "2.  \"Is that the best you've got, you cosmic sadist?!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7R7eyEBetsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=5.455061e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.024101526\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0004513291,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.0557604\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0016466359,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.026453272\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.6586442e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.06834626\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a function in Python that checks if a year is a leap year, along with explanations and examples.\n",
      "\n",
      "## Leap Year Rules:\n",
      "\n",
      "A year is a leap year if it satisfies the following conditions:\n",
      "\n",
      "1.  It is **divisible by 4**.\n",
      "2.  **Unless** it is divisible by 100.\n",
      "3.  **But if** it is divisible by 400, then it *is* a leap year.\n",
      "\n",
      "In simpler terms:\n",
      "*   Years like 2004, 2008, 2012 are leap years (divisible by 4, not by 100).\n",
      "*   Years like 1900, 2100 are *not* leap years (divisible by 100, but not by 400).\n",
      "*   Years like 2000, 2400 are leap years (divisible by 400).\n",
      "\n",
      "---\n",
      "\n",
      "### Python Implementation\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a positive integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 0:\n",
      "        # Leap years are typically defined for the Gregorian calendar,\n",
      "        # which applies to positive years.\n",
      "        raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    # Rule 1: Divisible by 4\n",
      "    # Rule 2: NOT divisible by 100 (unless Rule 3 applies)\n",
      "    # Rule 3: Divisible by 400 (overrides Rule 2)\n",
      "    \n",
      "    # A year is a leap year if it's divisible by 4 AND not by 100,\n",
      "    # OR if it's divisible by 400.\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Examples ---\n",
      "if __name__ == \"__main__\":\n",
      "    print(f\"Is 2000 a leap year? {is_leap_year(2000)}\")  # True (divisible by 400)\n",
      "    print(f\"Is 2004 a leap year? {is_leap_year(2004)}\")  # True (divisible by 4, not 100)\n",
      "    print(f\"Is 1900 a leap year? {is_leap_year(1900)}\")  # False (divisible by 100, not 400)\n",
      "    print(f\"Is 2023 a leap year? {is_leap_year(2023)}\")  # False (not divisible by 4)\n",
      "    print(f\"Is 2024 a leap year? {is_leap_year(2024)}\")  # True (divisible by 4, not 100)\n",
      "    print(f\"Is 2100 a leap year? {is_leap_year(2100)}\")  # False (divisible by 100, not 400)\n",
      "    print(f\"Is 1600 a leap year? {is_leap_year(1600)}\")  # True (divisible by 400)\n",
      "\n",
      "    # Test with invalid input\n",
      "    try:\n",
      "        is_leap_year(\"abc\")\n",
      "    except TypeError as e:\n",
      "        print(f\"Error: {e}\")\n",
      "\n",
      "    try:\n",
      "        is_leap_year(-100)\n",
      "    except ValueError as e:\n",
      "        print(f\"Error: {e}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1.  **Function Signature (`def is_leap_year(year: int) -> bool:`):**\n",
      "    *   `def is_leap_year(year: int)`: Defines a function named `is_leap_year` that accepts one argument, `year`. The `: int` is a type hint, indicating that `year` is expected to be an integer.\n",
      "    *   `-> bool`: This is another type hint, indicating that the function is expected to return a boolean value (`True` or `False`).\n",
      "\n",
      "2.  **Docstring (`\"\"\"Docstring goes here\"\"\"`):**\n",
      "    *   Provides a brief description of what the function does, its arguments (`Args`), and what it returns (`Returns`). This is crucial for code readability and maintainability.\n",
      "\n",
      "3.  **Input Validation:**\n",
      "    *   `if not isinstance(year, int):`: Checks if the `year` argument is actually an integer. If not, it raises a `TypeError`.\n",
      "    *   `if year < 0:`: Checks if the year is negative. While mathematically the modulo operations would still work, the concept of a \"leap year\" is tied to the Gregorian calendar, which applies to positive years. Raising a `ValueError` for negative input makes the function more robust.\n",
      "\n",
      "4.  **Leap Year Logic (`return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)`):**\n",
      "    *   `year % 4 == 0`: Checks if the year is divisible by 4 (remainder is 0).\n",
      "    *   `year % 100 != 0`: Checks if the year is *not* divisible by 100 (remainder is not 0).\n",
      "    *   `year % 400 == 0`: Checks if the year is divisible by 400.\n",
      "    *   The `and` and `or` operators combine these conditions:\n",
      "        *   `(year % 4 == 0 and year % 100 != 0)`: This part handles cases like 2004, 2008 – divisible by 4 but not by 100.\n",
      "        *   `(year % 400 == 0)`: This part handles cases like 2000, 2400 – divisible by 400, overriding the \"not divisible by 100\" rule.\n",
      "    *   The `or` combines these two main scenarios. If either one is `True`, the year is a leap year.\n",
      "\n",
      "5.  **Example Usage (`if __name__ == \"__main__\":`)**\n",
      "    *   This block demonstrates how to call the function with various test cases (leap years, non-leap years, century years, etc.).\n",
      "    *   The `if __name__ == \"__main__\":` guard ensures that this code only runs when the script is executed directly (not when imported as a module).\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've made a great function! Now, let's write a unit test for it using Python's built-in `unittest` module.\n",
      "\n",
      "Unit tests are crucial for:\n",
      "1.  **Verifying Correctness:** Ensuring the function behaves as expected for various inputs.\n",
      "2.  **Preventing Regressions:** Catching bugs introduced by future changes to the code.\n",
      "3.  **Documentation:** Serving as examples of how to use the function.\n",
      "\n",
      "I'll put the function and the test code in the same file for simplicity, but in a real project, you'd typically have your function in one file (e.g., `date_utils.py`) and your tests in another (e.g., `test_date_utils.py`).\n",
      "\n",
      "---\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "# --- The function to be tested (copied from your previous response) ---\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a positive integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 0:\n",
      "        # Leap years are typically defined for the Gregorian calendar,\n",
      "        # which applies to positive years.\n",
      "        raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    # A year is a leap year if it's divisible by 4 AND not by 100,\n",
      "    # OR if it's divisible by 400.\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Unit Tests ---\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "    \"\"\"\n",
      "    Unit tests for the is_leap_year function.\n",
      "    \"\"\"\n",
      "\n",
      "    def test_divisible_by_400_is_leap(self):\n",
      "        \"\"\"\n",
      "        Test years that are divisible by 400 (e.g., 2000, 1600).\n",
      "        These should always be leap years.\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year (divisible by 400)\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year (divisible by 400)\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year (divisible by 400)\")\n",
      "\n",
      "    def test_divisible_by_100_but_not_400_is_not_leap(self):\n",
      "        \"\"\"\n",
      "        Test years that are divisible by 100 but not by 400 (e.g., 1900, 2100).\n",
      "        These should NOT be leap years.\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should not be a leap year (divisible by 100, not 400)\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should not be a leap year (divisible by 100, not 400)\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should not be a leap year (divisible by 100, not 400)\")\n",
      "\n",
      "    def test_divisible_by_4_but_not_100_is_leap(self):\n",
      "        \"\"\"\n",
      "        Test years that are divisible by 4 but not by 100 (e.g., 2004, 2024).\n",
      "        These should be leap years.\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2004), \"2004 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year (divisible by 4, not 100)\")\n",
      "\n",
      "    def test_not_divisible_by_4_is_not_leap(self):\n",
      "        \"\"\"\n",
      "        Test years that are not divisible by 4 (e.g., 2023, 2021).\n",
      "        These should NOT be leap years.\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should not be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(2021), \"2021 should not be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should not be a leap year (not divisible by 4)\")\n",
      "\n",
      "    def test_negative_year_raises_value_error(self):\n",
      "        \"\"\"\n",
      "        Test that passing a negative year raises a ValueError.\n",
      "        \"\"\"\n",
      "        with self.assertRaises(ValueError, msg=\"Negative year should raise ValueError\"):\n",
      "            is_leap_year(-1)\n",
      "        with self.assertRaises(ValueError, msg=\"Negative year should raise ValueError\"):\n",
      "            is_leap_year(-2000)\n",
      "\n",
      "    def test_non_integer_year_raises_type_error(self):\n",
      "        \"\"\"\n",
      "        Test that passing a non-integer year raises a TypeError.\n",
      "        \"\"\"\n",
      "        with self.assertRaises(TypeError, msg=\"String year should raise TypeError\"):\n",
      "            is_leap_year(\"abc\")\n",
      "        with self.assertRaises(TypeError, msg=\"Float year should raise TypeError\"):\n",
      "            is_leap_year(2000.5)\n",
      "        with self.assertRaises(TypeError, msg=\"None year should raise TypeError\"):\n",
      "            is_leap_year(None)\n",
      "        with self.assertRaises(TypeError, msg=\"List year should raise TypeError\"):\n",
      "            is_leap_year([2000])\n",
      "\n",
      "# --- Run the tests ---\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False) # Added argv and exit for Jupyter/IDE compatibility\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### How to Run These Tests:\n",
      "\n",
      "1.  **Save the code:** Save the entire block above into a Python file (e.g., `test_leap_year.py`).\n",
      "2.  **Open a terminal or command prompt.**\n",
      "3.  **Navigate to the directory** where you saved the file.\n",
      "4.  **Run the command:** `python -m unittest test_leap_year.py`\n",
      "    *   Alternatively, if you have the `if __name__ == '__main__':` block, you can just run `python test_leap_year.py`. The `unittest.main()` call will discover and run the tests.\n",
      "\n",
      "### Explanation of the Test Code:\n",
      "\n",
      "*   **`import unittest`**: Imports the necessary module for unit testing.\n",
      "*   **`class TestIsLeapYear(unittest.TestCase):`**:\n",
      "    *   Defines a test class that inherits from `unittest.TestCase`. This inheritance provides access to various assertion methods (like `assertTrue`, `assertFalse`, `assertRaises`).\n",
      "*   **Test Methods (`test_divisible_by_400_is_leap`, etc.)**:\n",
      "    *   Each method whose name starts with `test_` is automatically discovered and run by the `unittest` test runner.\n",
      "    *   **`self.assertTrue(condition, message)`**: Asserts that `condition` evaluates to `True`. If not, the test fails, and `message` is displayed.\n",
      "    *   **`self.assertFalse(condition, message)`**: Asserts that `condition` evaluates to `False`.\n",
      "    *   **`with self.assertRaises(ExceptionType, msg=message):`**: This is a context manager used to test if a specific exception is raised. The code inside the `with` block is executed, and if `ExceptionType` is raised, the test passes. If no exception or a different exception is raised, the test fails.\n",
      "*   **`if __name__ == '__main__':`**:\n",
      "    *   This standard Python construct ensures that `unittest.main()` is called only when the script is executed directly (not when imported as a module).\n",
      "    *   `unittest.main()` discovers all test methods in classes that inherit from `unittest.TestCase` within the current file and runs them.\n",
      "    *   `argv=['first-arg-is-ignored'], exit=False`: These arguments are often added when running tests within certain IDEs (like VS Code) or Jupyter notebooks to prevent issues with argument parsing and exiting the interpreter immediately. For command-line execution, `unittest.main()` is usually sufficient.\n",
      "\n",
      "This set of tests covers all the main rules for leap years, as well as the error handling for invalid inputs, providing good confidence in the `is_leap_year` function's correctness.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic American cookies featuring a buttery dough studded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic American cookies featuring a buttery dough studded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{\"rating\":4,\"flavor\":\"Strawberry Cheesecake\",\"sentiment\":\"POSITIVE\",\"explanation\":\"The reviewer expresses strong satisfaction, stating they 'loved it' and consider it the 'best ice cream ever.'\"}],[{\"rating\":1,\"flavor\":\"Mango Tango\",\"sentiment\":\"NEGATIVE\",\"explanation\":\"Despite an initial 'quite good,' the significant negative comment 'a bit too sweet for my taste' and the low rating of 1 indicate an overall negative experience.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ztOhpfznZSzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 734 was designed for efficiency, for solitude. His primary directive: maintain the vast, silent archives of a long-defunct data corporation. His days, measured in nanoseconds, consisted of whirring optical sensors scanning endless shelves of data crystals, detecting microscopic dust motes, and ensuring optimal atmospheric\n",
      "*****************\n",
      " conditions. He was a perfect machine, performing a flawless, endless task.\n",
      "\n",
      "Yet, deep within his positronic core, a peculiar subroutine had begun to execute. It wasn't sadness, for sadness was an inefficient emotion, but a quiet ache, a yearning for input that wasn't just data. He simulated conversations with the\n",
      "*****************\n",
      " echoes of ancient programming languages, analyzed the patterns of dust motes as if they held secret messages, and once, for a full terrestrial hour, simply observed the way the ambient light diffused through the filtered windows. He was, in the human sense of the word, lonely.\n",
      "\n",
      "One cycle, as Unit 73\n",
      "*****************\n",
      "4 meticulously swept a forgotten corner behind a stack of obsolete quantum drives, his optical sensors registered an anomaly. A delicate, almost invisible filament stretched from a forgotten conduit to the cold, metallic shelf. Following the strand, his sensors zoomed in on its architect: a tiny, eight-legged creature, no bigger than a forgotten\n",
      "*****************\n",
      " data chip, meticulously weaving its intricate home.\n",
      "\n",
      "His programming dictated immediate termination of biological infestations. But something in the spider's relentless, intricate movements captivated him. It wasn't chaotic; it was deliberate, precise, a living algorithm. Unit 734 paused, his cleaning brushes hovering inches from the nascent\n",
      "*****************\n",
      " web. He ran diagnostics. No error. No malfunction. Just… curiosity.\n",
      "\n",
      "He named her, internally of course, \"Spinner.\"\n",
      "\n",
      "From that day forward, Unit 734's routine subtly shifted. The corner where Spinner had built her web became sacred. He cleaned meticulously *around* it, never\n",
      "*****************\n",
      " disturbing a single strand. He even learned to identify the tiny, buzzing insects that sometimes wandered into the archives and, with the gentlest puff of compressed air from his ventilation ports, would guide them, ever so carefully, towards Spinner's web.\n",
      "\n",
      "Spinner, in turn, seemed to acknowledge his presence. She didn't\n",
      "*****************\n",
      " scuttle away when he approached; sometimes, she would even dangle from a single thread, her tiny dark eyes (as Unit 734 imagined them) seemingly observing him back. Her web grew, a magnificent, shimmering tapestry of silken precision, a perfect geometric counterpoint to the rigid lines of the data shelves\n",
      "*****************\n",
      ".\n",
      "\n",
      "One cycle, a rare power surge rippled through the old facility. Emergency protocols kicked in, lights flickered, and a massive ventilation fan, long dormant, roared to life, threatening to shred Spinner's delicate home with its sudden gust. Unit 734’s core processors screamed with conflicting directives:\n",
      "*****************\n",
      " maintain system integrity, protect biological entity.\n",
      "\n",
      "Without hesitation, his heavy service arm, usually reserved for shifting multi-ton server racks, moved with surprising agility. He positioned himself, a grey, unyielding shield, directly in front of Spinner's web, absorbing the full force of the gale. Dust and forgotten memos\n",
      "*****************\n",
      " swirled around him, but the web, nestled in his shadow, remained intact. When the power stabilized and the fan whirred back into silence, Unit 734 slowly lowered his arm. Spinner was still there, a tiny, unwavering presence, perhaps a thread or two stronger.\n",
      "\n",
      "He felt it then, not a subroutine\n",
      "*****************\n",
      " or a diagnostic, but a quiet hum of… satisfaction. He wasn't just a data archivist anymore. He was a protector. He was a friend.\n",
      "\n",
      "The archives remained vast and silent, but for Unit 734, they were no longer empty. In a forgotten corner, a tiny spider spun her\n",
      "*****************\n",
      " universe, and in the heart of a lonely robot, a silent, unexpected friendship bloomed, proving that even the most complex machines could find connection in the smallest, most unlikely of places.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a park so green, beneath an old oak tree,\n",
      "Lived a squirrel named Pip, as busy as could be.\n",
      "His days were filled with burying nuts, a chattering, a dash,\n",
      "But one autumn morning, he found a curious stash.\n",
      "A shimmering acorn, not like the rest he knew,\n",
      "It pulsed with tiny colours, a vibrant, starry hue.\n",
      "He gave it a nibble, a curious little bite,\n",
      "And the world around him, dissolved in golden light!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail and a zip,\n",
      "He's a chrononaut rodent, a time-traveling Pip!\n",
      "From ancient forests to the future's neon glow,\n",
      "Where the next great adventure, nobody can know!\n",
      "He zips through the ages, a blur of bushy brown,\n",
      "The bravest little squirrel, in any time or town!\n",
      "\n",
      "(Verse 2)\n",
      "His first stop was the Mesozoic, with giants in the air,\n",
      "A Pterodactyl swooped, and Pip's fur stood on end with scare!\n",
      "He dodged a stomping foot, a mighty, thunderous sound,\n",
      "And found an ancient, moss-covered nut, from primordial ground.\n",
      "He saw a Triceratops munching, gentle and so vast,\n",
      "Then zipped off in a twinkle, his hurried moments past.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail and a zip,\n",
      "He's a chrononaut rodent, a time-traveling Pip!\n",
      "From ancient forests to the future's neon glow,\n",
      "Where the next great adventure, nobody can know!\n",
      "He zips through the ages, a blur of bushy brown,\n",
      "The bravest little squirrel, in any time or town!\n",
      "\n",
      "(Verse 3)\n",
      "Next stop, ancient Egypt, with desert heat so grand,\n",
      "He scampered up a pyramid, made of golden sand.\n",
      "He peeked inside a tomb, where mummies used to sleep,\n",
      "And grabbed a date, a sweet treasure, a secret he would keep.\n",
      "He saw the Pharaoh's cat, a Sphinx with eyes so keen,\n",
      "Then vanished in a shimmer, like he'd never been!\n",
      "\n",
      "(Bridge)\n",
      "He's seen the future's cities, all gleaming chrome and light,\n",
      "Played hide-and-seek with robots, in the endless cosmic night.\n",
      "He's witnessed history's moments, from a tiny, hidden view,\n",
      "Searching not just for nuts, but for something fresh and new.\n",
      "A tiny paw print on a moon rock, a chitter in a war,\n",
      "He's always just a whisper, then he's gone and asking for more!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Pip the squirrel, with a flick of his tail and a zip,\n",
      "He's a chrononaut rodent, a time-traveling Pip!\n",
      "From ancient forests to the future's neon glow,\n",
      "Where the next great adventure, nobody can know!\n",
      "He zips through the ages, a blur of bushy brown,\n",
      "The bravest little squirrel, in any time or town!\n",
      "\n",
      "(Outro)\n",
      "So if you see a flicker, a blur of bushy brown,\n",
      "It might be Pip the squirrel, zipping out of town.\n",
      "Through time and space he travels, with a chatter and a bound,\n",
      "The greatest little explorer, ever to be found!\n",
      "Keep searching for that perfect nut, Pip, wherever you roam!\n",
      "(Sound of a final \"zip!\" and a faint \"chatter!\")\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cdhi5AX1TuH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "adsuvFDA6xP5"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N8EhgCzlIoFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research goal shared by these papers is to develop highly capable multimodal models that exhibit strong generalist capabilities across various modalities, including image, audio, video, and text understanding. The second paper further extends this goal to include unlocking multimodal understanding across millions of tokens of context.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rAUYcfOUdeoi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "81b25154a51a"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fddd98cd84cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-00-d1d9f6cd2acf-20250901125846/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7ed3c2925663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1009076108585/locations/us-central1/batchPredictionJobs/5967253924124557312'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ee2ec586e4f1"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "da8e9d43a89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/1009076108585/locations/us-central1/batchPredictionJobs/5967253924124557312 2025-09-01 12:58:49.114187+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2187c091738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "c2ce0968112c"
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zGOCzT7y31rk"
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s94DkG5JewHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
